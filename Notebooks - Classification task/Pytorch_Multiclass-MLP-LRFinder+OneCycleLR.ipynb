{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "# Third-party library imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch and related librarie\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opti\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# einops library for tensor operations\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "# Custom TINTO library imports\n",
    "from TINTOlib.tinto import TINTO\n",
    "from TINTOlib.supertml import SuperTML\n",
    "from TINTOlib.igtd import IGTD\n",
    "from TINTOlib.refined import REFINED\n",
    "from TINTOlib.barGraph import BarGraph\n",
    "from TINTOlib.distanceMatrix import DistanceMatrix\n",
    "from TINTOlib.combination import Combination\n",
    "from TINTOlib.featureWrap import FeatureWrap\n",
    "from TINTOlib.bie import BIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get CUDA version\n",
    "cuda_version = torch.version.cuda\n",
    "print(f\"CUDA Version: {cuda_version}\")\n",
    "\n",
    "# Get cuDNN version\n",
    "cudnn_version = torch.backends.cudnn.version()\n",
    "print(f\"cuDNN Version: {cudnn_version}\")\n",
    "\n",
    "# Get PyTorch version\n",
    "pytorch_version = torch.__version__\n",
    "print(f\"PyTorch Version: {pytorch_version}\")\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. PyTorch can use GPU.\")\n",
    "    \n",
    "    # Get the name of the current GPU\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Create a random tensor and move it to GPU to verify\n",
    "    x = torch.rand(5, 3)\n",
    "    print(f\"Is this tensor on GPU? {x.cuda().is_cuda}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. PyTorch will use CPU.\")\n",
    "\n",
    "# Additional check: is CUDA initialized?\n",
    "print(f\"Is CUDA initialized? {torch.cuda.is_initialized()}\")\n",
    "\n",
    "# Number of available GPUs\n",
    "print(f\"Number of available GPUs: {torch.cuda.device_count()}\")\n",
    "\n",
    "# Current device index\n",
    "print(f\"Current device index: {torch.cuda.current_device()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 64\n",
    "# SET RANDOM SEED FOR REPRODUCIBILITY\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable to store dataset name\n",
    "dataset_name = 'gas'\n",
    "results_path = f'logs/Multiclass/{dataset_name}/MLP_Multiclass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"../Datasets_benchmark/Multiclass/{dataset_name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD AND PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get last column number of unique values\n",
    "n_classes = df.iloc[:,-1].nunique()\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=32):\n",
    "    \n",
    "    # Generate the images if the folder does not exist\n",
    "    # if not os.path.exists(images_folder):\n",
    "    #     #Generate the images\n",
    "    #     image_model.generateImages(df, images_folder)\n",
    "    # else:\n",
    "    #     print(\"The images are already generated\")\n",
    "\n",
    "    # img_paths = os.path.join(images_folder,problem_type+\".csv\")\n",
    "\n",
    "    # print(img_paths)\n",
    "\n",
    "    # imgs = pd.read_csv(img_paths)\n",
    "\n",
    "    # Update image paths\n",
    "    # imgs[\"images\"] = images_folder + \"/\" + imgs[\"images\"]\n",
    "\n",
    "    # Combine datasets\n",
    "    # combined_dataset = pd.concat([imgs, df], axis=1)\n",
    "\n",
    "    # Split data\n",
    "    df_x = df.drop(df.columns[-1], axis=1) # .drop(\"class\", axis=1)\n",
    "    df_y = df[df.columns[-1]]  # \"class\"\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(df_x, df_y, test_size=0.20, random_state=SEED)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.50, random_state=SEED)\n",
    "    \n",
    "    # Create a MinMaxScaler object\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    # Scale numerical data\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n",
    "    X_val = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)\n",
    "\n",
    "    # ONE HOT ENCODING FOR MULTICLASS PROBLEMS\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    label_encoder.fit(y_train)\n",
    "\n",
    "    # Transform the target variable using one-hot encoding\n",
    "    y_train = label_encoder.transform(y_train)\n",
    "    y_val = label_encoder.transform(y_val)\n",
    "    y_test = label_encoder.transform(y_test)\n",
    "\n",
    "    attributes = len(X_train.columns)\n",
    "    # height, width, channels = None  # Previously: X_train_img[0].shape\n",
    "    imgs_shape = None  # Previously: (channels, height, width)\n",
    "    \n",
    "    print(\"Attributes: \", attributes)\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.as_tensor(X_train.values, dtype=torch.float32)\n",
    "    X_val_tensor = torch.as_tensor(X_val.values, dtype=torch.float32)\n",
    "    X_test_tensor = torch.as_tensor(X_test.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.as_tensor(y_train, dtype=torch.long)\n",
    "    y_val_tensor = torch.as_tensor(y_val, dtype=torch.long)\n",
    "    y_test_tensor = torch.as_tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL ARCHITECTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, attributes):\n",
    "        super(Model1, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(attributes, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, n_classes)  # Output layer for regression\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, attributes):\n",
    "        super(Model2, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(attributes, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(16, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Model3(nn.Module):\n",
    "    def __init__(self, attributes, dropout=0.1):\n",
    "        super(Model3, self).__init__()\n",
    "        # MLP branch\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(attributes, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "        # MLP branch 2\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(attributes, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "        \n",
    "        self.final_layers = nn.Sequential(\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, n_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        branch1_output = self.mlp(x)\n",
    "        branch2_output = self.mlp2(x)\n",
    "        merged = torch.cat((branch1_output, branch2_output), dim=1)\n",
    "        return self.final_layers(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COMPILE AND FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "def compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, batch_size=32, epochs=100, min_lr=1e-3, max_lr=1, device='cuda', weight_decay=1e-2):\n",
    "    model = model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=min_lr, weight_decay=weight_decay)\n",
    "    \n",
    "    total_steps = epochs * len(train_loader)\n",
    "    scheduler = OneCycleLR(optimizer, max_lr=max_lr, div_factor=max_lr/min_lr, total_steps=total_steps, pct_start=0.3, final_div_factor=1)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = 20\n",
    "    best_model = None\n",
    "    best_epoch = 0\n",
    "    warm_up_epochs = epochs*0.3\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_accuracy': [], 'val_accuracy': [], 'train_f1': [], 'val_f1': [], 'learning_rate': [], 'epoch_time': []}\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_targets = []\n",
    "        for num_data, targets in train_loader:\n",
    "            num_data, targets = num_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(num_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "            train_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for num_data, targets in val_loader:\n",
    "                num_data, targets = num_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "                outputs = model(num_data)\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Get the current learning rate\n",
    "        current_lr = scheduler.get_last_lr()\n",
    "        \n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            best_epoch = epoch + 1\n",
    "            #early_stopping_counter = 0\n",
    "        #else:\n",
    "            #if epoch > warm_up_epochs:\n",
    "                #early_stopping_counter += 1\n",
    "                #if early_stopping_counter >= early_stopping_patience:\n",
    "                    #print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                    #break\n",
    "\n",
    "        train_accuracy = accuracy_score(train_targets, train_predictions)\n",
    "        train_f1 = f1_score(train_targets, train_predictions, average='weighted')\n",
    "        val_accuracy = accuracy_score(val_targets, val_predictions)\n",
    "        val_f1 = f1_score(val_targets, val_predictions, average='weighted')\n",
    "\n",
    "        epoch_time = time.time() - epoch_start_time\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_accuracy'].append(train_accuracy)\n",
    "        history['val_accuracy'].append(val_accuracy)\n",
    "        history['train_f1'].append(train_f1)\n",
    "        history['val_f1'].append(val_f1)\n",
    "        history['learning_rate'].append(current_lr)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    # Calculate and save metrics\n",
    "    train_metrics = calculate_metrics(model, train_loader, device)\n",
    "    val_metrics = calculate_metrics(model, val_loader, device)\n",
    "    test_metrics = calculate_metrics(model, test_loader, device)\n",
    "\n",
    "    metrics = {\n",
    "        'train_loss': train_metrics['loss'],\n",
    "        'train_accuracy': train_metrics['accuracy'],\n",
    "        'train_precision': train_metrics['precision'],\n",
    "        'train_recall': train_metrics['recall'],\n",
    "        'train_f1': train_metrics['f1'],\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'val_accuracy': val_metrics['accuracy'],\n",
    "        'val_precision': val_metrics['precision'],\n",
    "        'val_recall': val_metrics['recall'],\n",
    "        'val_f1': val_metrics['f1'],\n",
    "        'test_loss': test_metrics['loss'],\n",
    "        'test_accuracy': test_metrics['accuracy'],\n",
    "        'test_precision': test_metrics['precision'],\n",
    "        'test_recall': test_metrics['recall'],\n",
    "        'test_f1': test_metrics['f1'],\n",
    "        'min_lr': min_lr,\n",
    "        'max_lr': max_lr,\n",
    "        'total_time': total_time,\n",
    "        'average_epoch_time': sum(history['epoch_time']) / len(history['epoch_time'])\n",
    "    }\n",
    "\n",
    "    print(f\"\\nTraining completed in {total_time:.2f} seconds\")\n",
    "    print(f\"Best model found at epoch {best_epoch}/{epochs}\")\n",
    "    print(f\"Best Train Loss: {history['train_loss'][best_epoch-1]:.4f}, Best Val Loss: {history['val_loss'][best_epoch-1]:.4f}\")\n",
    "    print(f\"Best Train Accuracy: {history['train_accuracy'][best_epoch-1]:.4f}, Best Val Accuracy: {history['val_accuracy'][best_epoch-1]:.4f}\")\n",
    "    print(f\"Best Train F1: {history['train_f1'][best_epoch-1]:.4f}, Best Val F1: {history['val_f1'][best_epoch-1]:.4f}\")\n",
    "\n",
    "    # Save figures for this fold\n",
    "    os.makedirs(f\"models/Multiclass/{dataset_name}/MLP/{model_name}\", exist_ok=True)\n",
    "    plot_metric(history['train_loss'], history['val_loss'], 'Loss', dataset_name, model_name)\n",
    "    plot_metric(history['train_accuracy'], history['val_accuracy'], 'Accuracy', dataset_name, model_name)\n",
    "    plot_metric(history['train_f1'], history['val_f1'], 'F1 Score', dataset_name, model_name)\n",
    "    plot_learning_rate(history['learning_rate'], dataset_name, model_name)\n",
    "\n",
    "    # Save metrics to a file\n",
    "    os.makedirs(f'logs/Multiclass/{dataset_name}/MLP/{model_name}', exist_ok=True)\n",
    "    with open(f'logs/Multiclass/{dataset_name}/MLP/{model_name}/metrics.txt', 'w') as f:\n",
    "        for key, value in metrics.items():\n",
    "            f.write(f'{key}: {value}\\n')\n",
    "            \n",
    "    # Save best model\n",
    "    model_save_path = f\"models/Multiclass/{dataset_name}/MLP/{model_name}/best_model.pth\"\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "    torch.save(best_model, model_save_path)\n",
    "    print(f\"Best model saved to {model_save_path}\")\n",
    "            \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics \n",
    "\n",
    "def calculate_metrics(model, data_loader, device):\n",
    "    model.eval()\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for num_data, targets in data_loader:\n",
    "            num_data, targets = num_data.to(device, non_blocking=True), targets.to(device, non_blocking=True)\n",
    "            outputs = model(num_data)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predictions.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
    "\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    precision = precision_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_targets, all_predictions, average='weighted', zero_division=0)\n",
    "\n",
    "    return {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def plot_metric(train_metric, val_metric, metric_name, dataset_name, model_name):\n",
    "    plt.figure()\n",
    "    plt.plot(train_metric, label=f'Train {metric_name}')\n",
    "    plt.plot(val_metric, label=f'Validation {metric_name}')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.title(f'{metric_name} vs. Epoch')\n",
    "    plt.savefig(f\"models/Multiclass/{dataset_name}/MLP/{model_name}/{metric_name.lower()}_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_learning_rate(learning_rates, dataset_name, model_name):\n",
    "    plt.figure()\n",
    "    plt.plot(learning_rates)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learning Rate')\n",
    "    plt.title('Learning Rate vs. Epoch')\n",
    "    plt.savefig(f\"models/Multiclass/{dataset_name}/MLP/{model_name}/learning_rate_plot.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def safe_compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, batch_size=64, epochs=100, min_lr=1e-3, max_lr=1 , device='cuda', weight_decay=1e-2):\n",
    "    try:\n",
    "        if model is None:\n",
    "            print(f\"Model {model_name} is None\")\n",
    "            return None\n",
    "        else:\n",
    "            # Compile and fit the model\n",
    "            metrics = compile_and_fit(model, train_loader, val_loader, test_loader, dataset_name, model_name, epochs=epochs, min_lr=min_lr, max_lr=max_lr, device=device, weight_decay=weight_decay)\n",
    "            return metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to compile and fit {model_name}: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "    finally:\n",
    "        # Clear CUDA cache and force garbage collection\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import traceback\n",
    "\n",
    "def try_create_model(model_class, attributes):\n",
    "    try:\n",
    "        model = model_class(attributes)\n",
    "        \n",
    "        # Test the model with a sample input\n",
    "        num_input = torch.randn(4, attributes)\n",
    "        output = model(num_input)\n",
    "        \n",
    "        print(f\"Successfully created and tested {model_class.__name__}\")\n",
    "        \n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating or testing {model_class.__name__}:\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "def run_lr_finder(model_class, attributes, imgs_shape, dataset_name, name, train_loader, val_loader, num_iter):\n",
    "\n",
    "    # Define the path where the plot will be saved\n",
    "    save_dir = os.path.join(f\"logs/Multiclass/{dataset_name}/MLP/{name}\")\n",
    "    save_path = os.path.join(save_dir, 'lr_finder_plot.png')\n",
    "\n",
    "    # Check if the file already exists\n",
    "    if not os.path.exists(save_path):\n",
    "        # Create and train Model\n",
    "        model = try_create_model(model_class, attributes)\n",
    "        \n",
    "        if model is None:\n",
    "            return None\n",
    "        \n",
    "        # Move model to device\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        \n",
    "        optimizer = optim.AdamW(model.parameters(), lr=1e-7, weight_decay=0.0001)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        lr_finder = LRFinder(model, optimizer, criterion, device=device)\n",
    "        lr_finder.range_test(train_loader, val_loader=val_loader, end_lr=1, num_iter=num_iter, step_mode=\"exp\")\n",
    "        \n",
    "        axis, lr = lr_finder.plot()\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Get the figure from the axis and save it\n",
    "        fig = axis.figure\n",
    "        fig.savefig(save_path)\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "        \n",
    "        # Close the figure to ensure it's saved properly\n",
    "        plt.close(fig)\n",
    "        \n",
    "        lr_finder.reset()\n",
    "        print(f\"Suggested learning rate: {lr}\")\n",
    "        \n",
    "        return lr\n",
    "    else:\n",
    "        print(f\"LR finder plot already exists at {save_path}. Skipping LR finder process.\")\n",
    "        # Load and display the existing image\n",
    "        img = plt.imread(save_path)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Turn off axis numbers and ticks\n",
    "        plt.title(\"Learning Rate Finder Plot\")\n",
    "        plt.show()\n",
    "        \n",
    "        return None  # Or you could return a default learning rate here\n",
    "\n",
    "# Usage example:\n",
    "# lr = run_lr_finder(Model1, attributes, imgs_shape, dataset_name, name, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPERIMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, random_seed=SEED)\n",
    "#image_model = REFINED(problem= problem_type,hcIterations=5)\n",
    "#image_model = IGTD(problem= problem_type)\n",
    "#image_model = BarGraph(problem= problem_type)\n",
    "#image_model = DistanceMatrix(problem= problem_type)\n",
    "#image_model = Combination(problem= problem_type)\n",
    "#image_model = SuperTML(problem= problem_type)\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_TINTO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iterations_per_epoch(dataset_size, batch_size):\n",
    "    iterations = dataset_size // batch_size\n",
    "    if dataset_size % batch_size != 0:\n",
    "        iterations += 1\n",
    "    return iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = calculate_iterations_per_epoch(df.shape[0], batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### EXPERIMENT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select the model and the parameters\n",
    "problem_type = \"supervised\"\n",
    "image_model = TINTO(problem= problem_type, blur=True, random_seed=SEED)\n",
    "name = f\"\"\n",
    "\n",
    "#Define the dataset path and the folder where the images will be saved\n",
    "images_folder = f\"../HyNNImages/Multiclass/{dataset_name}/images_{dataset_name}_{name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, attributes, imgs_shape, label_encoder = load_and_preprocess_data(images_folder, image_model, problem_type, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model1, attributes, imgs_shape, dataset_name, f\"Model1\", train_loader, val_loader, num_iter=int(num_epochs/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model1\n",
    "model1 = try_create_model(Model1, attributes)  # Attempt to create Model1\n",
    "model1_metrics = safe_compile_and_fit(model1, train_loader, val_loader, test_loader, dataset_name, f\"Model1\", min_lr=5e-5, max_lr=2e-2)  # Train and evaluate Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model2, attributes, imgs_shape, dataset_name, f\"Model2\", train_loader, val_loader, num_iter=int(num_epochs/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model2\n",
    "model2 = try_create_model(Model2, attributes)  # Attempt to create Model2\n",
    "model2_metrics = safe_compile_and_fit(model2, train_loader, val_loader, test_loader, dataset_name, f\"Model2\", min_lr=1e-5, max_lr=4e-3)  # Train and evaluate Model2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = run_lr_finder(Model3, attributes, imgs_shape, dataset_name, f\"Model3\", train_loader, val_loader, num_iter=int(num_epochs/4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Model3\n",
    "model3 = try_create_model(Model3, attributes)  # Attempt to create Model3\n",
    "model3_metrics = safe_compile_and_fit(model3, train_loader, val_loader, test_loader, dataset_name, f\"Model3\", min_lr=1e-5, max_lr=2e-3)  # Train and evaluate Model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison of metrics only for models that ran successfully\n",
    "if model1_metrics:\n",
    "    print(\"Model 1 Metrics:\", model1_metrics)  # Print metrics for Model1 if available\n",
    "if model2_metrics:\n",
    "    print(\"Model 2 Metrics:\", model2_metrics)  # Print metrics for Model2 if available\n",
    "if model3_metrics:\n",
    "    print(\"Model 3 Metrics:\", model3_metrics)  # Print metrics for Model3 if available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINAL METRICS AND BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_model(base_path):\n",
    "    best_accuracy = float('-inf')\n",
    "    best_folder = None\n",
    "\n",
    "    # Walk through all directories and files in the base path\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        for file in files:\n",
    "            if file == f'metrics.txt':\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Read metrics from the file\n",
    "                with open(file_path, 'r') as f:\n",
    "                    metrics = f.read()\n",
    "                \n",
    "                # Parse the metrics into a dictionary\n",
    "                metrics_dict = {}\n",
    "                for line in metrics.splitlines():\n",
    "                    key, value = line.split(': ')\n",
    "                    metrics_dict[key.strip()] = float(value.strip())\n",
    "                \n",
    "                # Check if the current folder has a better validation loss\n",
    "                if metrics_dict['test_accuracy'] > best_accuracy:\n",
    "                    best_accuracy = metrics_dict['test_accuracy']\n",
    "                    best_folder = root\n",
    "    \n",
    "    return best_folder, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_metrics(file_path):\n",
    "    metrics = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            key, value = line.split(': ')\n",
    "            metrics[key.strip()] = float(value.strip())\n",
    "    return metrics\n",
    "\n",
    "def rename_folder(old_folder_path, prefix):\n",
    "    folder_name = os.path.basename(old_folder_path)\n",
    "    new_folder_name = f\"{prefix}_{folder_name}\"\n",
    "    parent_dir = os.path.dirname(old_folder_path)\n",
    "    new_folder_path = os.path.join(parent_dir, new_folder_name)\n",
    "    os.rename(old_folder_path, new_folder_path)\n",
    "    return new_folder_path\n",
    "\n",
    "def process_folders(root_dir):\n",
    "    prefixes = [\"Model\"]\n",
    "    best_folders = []\n",
    "\n",
    "    for prefix in prefixes:\n",
    "        matching_folders = [f for f in os.listdir(root_dir) if f.startswith(prefix) and os.path.isdir(os.path.join(root_dir, f))]\n",
    "        if matching_folders:\n",
    "            best_folder = None\n",
    "            best_test_accuracy = float('-inf')\n",
    "            for folder in matching_folders:\n",
    "                metrics_file = os.path.join(root_dir, folder, 'metrics.txt')\n",
    "                if os.path.exists(metrics_file):\n",
    "                    metrics = read_metrics(metrics_file)\n",
    "                    if metrics['test_accuracy'] > best_test_accuracy:\n",
    "                        best_test_accuracy = metrics['test_accuracy']\n",
    "                        best_folder = folder\n",
    "            if best_folder:\n",
    "                new_path = rename_folder(os.path.join(root_dir, best_folder), \"TOP\")\n",
    "                best_folders.append(new_path)\n",
    "    \n",
    "    if best_folders:\n",
    "        overall_best_folder = None\n",
    "        overall_best_test_accuracy = float('-inf')\n",
    "        for folder in best_folders:\n",
    "            metrics_file = os.path.join(folder, 'metrics.txt')\n",
    "            if os.path.exists(metrics_file):\n",
    "                metrics = read_metrics(metrics_file)\n",
    "                if metrics['test_accuracy'] > overall_best_test_accuracy:\n",
    "                    overall_best_test_accuracy = metrics['test_accuracy']\n",
    "                    overall_best_folder = folder\n",
    "        if overall_best_folder:\n",
    "            rename_folder(overall_best_folder, \"BEST\")\n",
    "        \n",
    "    return best_folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "base_path = f\"logs/Multiclass/{dataset_name}/MLP\"\n",
    "best_folders = process_folders(base_path)\n",
    "print(f\"Best model folder: {best_folders}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
